{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOs0Bl54zoXmmQJHVd/ryhB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepu-Sharma/Review/blob/main/JJ_single_and_array.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiNlnCut_4T0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.integrate import solve_ivp\n",
        "from scipy.signal import find_peaks\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Aesthetic Settings for Plots ---\n",
        "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 10\n",
        "plt.rcParams['xtick.labelsize'] = 10\n",
        "plt.rcParams['ytick.labelsize'] = 10\n",
        "\n",
        "# --- Physical Constants ---\n",
        "HBAR = 1.0545718e-34              # Reduced Planck's constant (J·s)\n",
        "ELECTRON_CHARGE = 1.60217662e-19   # Elementary charge (C)\n",
        "FLUX_QUANTUM_HALF = HBAR / (2 * ELECTRON_CHARGE)  # Reduced flux quantum (V·s)\n",
        "FLUX_QUANTUM = HBAR * np.pi / ELECTRON_CHARGE      # Magnetic flux quantum (Wb)\n",
        "JOSEPHSON_CONSTANT = 2 * ELECTRON_CHARGE / HBAR     # Josephson constant (Hz/V)\n",
        "\n",
        "# --- Josephson Junction Parameters ---\n",
        "CRITICAL_CURRENT = 1e-6    # Critical current (A)\n",
        "CAPACITANCE = 1e-12        # Capacitance (F)\n",
        "SHUNT_RESISTANCE = 50      # Shunt resistance (Ω)\n",
        "\n",
        "# --- Simulation Parameters ---\n",
        "SIMULATION_TIME = 20e-9    # Extended simulation duration: 20 ns\n",
        "dt = 5e-14                 # Time step (s)\n",
        "TIME_SPAN = (0, SIMULATION_TIME)\n",
        "TIME_EVALUATION_POINTS = np.linspace(0, SIMULATION_TIME, int(SIMULATION_TIME / dt))\n",
        "\n",
        "# --- Define a Ramped Bias Current ---\n",
        "# Bias current ramps linearly, reaching about 5×I_c at end.\n",
        "RAMP_RATE = (5 * CRITICAL_CURRENT) / SIMULATION_TIME  # in A/s\n",
        "\n",
        "def bias_current(t):\n",
        "    \"\"\"\n",
        "    Ramped bias current as a function of time.\n",
        "\n",
        "    Input:\n",
        "      t : time (s)\n",
        "    Returns:\n",
        "      Bias current (A)\n",
        "    \"\"\"\n",
        "    return RAMP_RATE * t\n",
        "\n",
        "# --- Stochastic Noise Function ---\n",
        "# Increase noise amplitude; here we set noise_amp = 1e-6 A (equal to I_c)\n",
        "noise_amp = 1e-6\n",
        "# Pre-generate noise on a dense time grid:\n",
        "noise_time_grid = np.linspace(0, SIMULATION_TIME, 10000)\n",
        "noise_values = np.random.normal(loc=0.0, scale=noise_amp, size=noise_time_grid.shape)\n",
        "def noise_func(t):\n",
        "    \"\"\"Return the noise value at time t via linear interpolation.\"\"\"\n",
        "    return np.interp(t, noise_time_grid, noise_values)\n",
        "\n",
        "# --- Enhanced RCSJ Model with Nonlinear Adaptation and Noise ---\n",
        "def rcsj_adapt_nonlin(t, state, critical_current, capacitance, shunt_resistance,\n",
        "                      flux_quantum_half, tau_adapt, k_adapt):\n",
        "    \"\"\"\n",
        "    Enhanced RCSJ model that includes stochastic noise and nonlinear adaptation.\n",
        "\n",
        "    State Variables:\n",
        "      state[0] = φ (phase)\n",
        "      state[1] = V (voltage)\n",
        "      state[2] = I_adapt (adaptation current)\n",
        "\n",
        "    Equations:\n",
        "      dφ/dt = V / FLUX_QUANTUM_HALF\n",
        "\n",
        "      dV/dt = ( I_bias(t) - I_adapt - critical_current*sin(φ) - V/shunt_resistance )/capacitance\n",
        "              + noise_func(t)\n",
        "\n",
        "      dI_adapt/dt = ( k_adapt * tanh(V) - I_adapt )/tau_adapt\n",
        "\n",
        "    The tanh nonlinearity provides a saturating synaptic-like effect.\n",
        "    \"\"\"\n",
        "    phi, voltage, I_adapt = state\n",
        "    Ib = bias_current(t)\n",
        "\n",
        "    dphi_dt = voltage / flux_quantum_half\n",
        "    dvoltage_dt = (Ib - I_adapt - critical_current * np.sin(phi) - voltage / shunt_resistance) / capacitance \\\n",
        "                  + noise_func(t)\n",
        "    dI_adapt_dt = (k_adapt * np.tanh(voltage) - I_adapt) / tau_adapt\n",
        "    return [dphi_dt, dvoltage_dt, dI_adapt_dt]\n",
        "\n",
        "# --- Adaptation Parameters ---\n",
        "tau_adapt = 1e-9  # Adaptation time constant (s)\n",
        "k_adapt = 0.05    # Adaptation strength (A/V)\n",
        "\n",
        "# --- Solve the Differential Equations ---\n",
        "initial_state = [0, 0, 0]  # Initial phase, voltage, and adaptation current\n",
        "solution = solve_ivp(rcsj_adapt_nonlin, TIME_SPAN, initial_state, method='RK45',\n",
        "                     t_eval=TIME_EVALUATION_POINTS,\n",
        "                     args=(CRITICAL_CURRENT, CAPACITANCE, SHUNT_RESISTANCE,\n",
        "                           FLUX_QUANTUM_HALF, tau_adapt, k_adapt),\n",
        "                     atol=1e-12, rtol=1e-9)\n",
        "\n",
        "# Retrieve simulation results\n",
        "time = solution.t                         # Time array (s)\n",
        "phi = solution.y[0]                       # Phase (rad)\n",
        "voltage = solution.y[1]                   # Voltage (V)\n",
        "I_adapt = solution.y[2]                   # Adaptation current (A)\n",
        "bias_values = bias_current(time)          # Deterministic bias (A)\n",
        "\n",
        "# --- Compute Voltage Derivative ---\n",
        "voltage_derivative = np.gradient(voltage, time)\n",
        "\n",
        "# --- Spike Detection via Voltage Derivative ---\n",
        "# Focus on times when bias exceeds critical current.\n",
        "active_mask = bias_values >= CRITICAL_CURRENT\n",
        "active_time = time[active_mask]\n",
        "active_deriv = voltage_derivative[active_mask]\n",
        "active_bias = bias_values[active_mask]\n",
        "\n",
        "if active_deriv.size > 0:\n",
        "    deriv_threshold = 0.75 * np.max(active_deriv)\n",
        "    peak_indices, _ = find_peaks(active_deriv, height=deriv_threshold, distance=20)\n",
        "else:\n",
        "    peak_indices = np.array([])\n",
        "\n",
        "spike_times = active_time[peak_indices]\n",
        "bias_at_spikes = active_bias[peak_indices]\n",
        "\n",
        "# --- Exclude Early Transients (e.g., first 5 ns) ---\n",
        "t_transient = 5e-9\n",
        "steady_mask = spike_times > t_transient\n",
        "steady_spike_times = spike_times[steady_mask]\n",
        "steady_bias = bias_at_spikes[steady_mask]\n",
        "\n",
        "if len(steady_spike_times) > 1:\n",
        "    spike_intervals = np.diff(steady_spike_times)\n",
        "    instantaneous_frequency = 1 / spike_intervals  # in Hz\n",
        "    bias_for_frequency = steady_bias[1:]\n",
        "    mid_spike_times = (steady_spike_times[:-1] + steady_spike_times[1:]) / 2.0\n",
        "else:\n",
        "    spike_intervals = np.array([])\n",
        "    instantaneous_frequency = np.array([])\n",
        "    bias_for_frequency = np.array([])\n",
        "    mid_spike_times = np.array([])\n",
        "\n",
        "if steady_spike_times.size > 0:\n",
        "    estimated_threshold_current = bias_current(steady_spike_times[0])\n",
        "else:\n",
        "    estimated_threshold_current = np.nan\n",
        "\n",
        "# --- Additional Derived Measures ---\n",
        "if spike_intervals.size > 0:\n",
        "    mean_isi = np.mean(spike_intervals)\n",
        "    std_isi = np.std(spike_intervals)\n",
        "    mean_freq = np.mean(instantaneous_frequency)\n",
        "else:\n",
        "    mean_isi = std_isi = mean_freq = np.nan\n",
        "\n",
        "# Effective drive: bias minus adaptation current.\n",
        "I_effective = bias_values - I_adapt\n",
        "\n",
        "# --- Print Simulation Statistics ---\n",
        "print(\"Enhanced JJ (Neuron-inspired) with Nonlinear Adaptation and Stochastic Noise:\")\n",
        "print(f\"Simulation Time: {SIMULATION_TIME*1e9:.1f} ns\")\n",
        "print(f\"Max dV/dt: {np.max(np.abs(voltage_derivative)):.3e} V/s\")\n",
        "print(f\"Estimated Threshold Current (steady-state, first spike): {estimated_threshold_current:.3e} A\")\n",
        "print(f\"Number of Steady-State Detected Spikes: {len(steady_spike_times)}\")\n",
        "if instantaneous_frequency.size:\n",
        "    print(f\"Average Instantaneous Frequency: {mean_freq:.3e} Hz\")\n",
        "    print(f\"ISI: Mean = {mean_isi*1e9:.3f} ns, Std = {std_isi*1e9:.3f} ns\")\n",
        "else:\n",
        "    print(\"No steady-state spikes detected for frequency analysis.\")\n",
        "print(f\"Effective Drive at final time: {I_effective[-1]*1e6:.3f} μA\")\n",
        "\n",
        "# --- Plotting ---\n",
        "# Figure 1: Main Time Series\n",
        "fig1, axes1 = plt.subplots(8, 1, figsize=(10, 32), dpi=150, facecolor='white')\n",
        "\n",
        "# 1. Input Bias Current\n",
        "axes1[0].plot(time*1e9, bias_values*1e6, color='royalblue', linewidth=2, label='Bias Current')\n",
        "axes1[0].set_xlabel('Time (ns)')\n",
        "axes1[0].set_ylabel('Current (μA)')\n",
        "axes1[0].set_title('Ramped Input Bias Current')\n",
        "axes1[0].grid(True, linestyle='--', alpha=0.7)\n",
        "axes1[0].legend(loc='upper left')\n",
        "\n",
        "# 2. Adaptation Current\n",
        "axes1[1].plot(time*1e9, I_adapt*1e6, color='teal', linewidth=2, label='Adaptation Current')\n",
        "axes1[1].set_xlabel('Time (ns)')\n",
        "axes1[1].set_ylabel('I_{adapt} (μA)')\n",
        "axes1[1].set_title('Adaptation Current Evolution')\n",
        "axes1[1].grid(True, linestyle='--', alpha=0.7)\n",
        "axes1[1].legend(loc='upper left')\n",
        "\n",
        "# 3. Effective Drive (Bias - Adaptation)\n",
        "axes1[2].plot(time*1e9, I_effective*1e6, color='darkmagenta', linewidth=2, label='Effective Drive')\n",
        "axes1[2].set_xlabel('Time (ns)')\n",
        "axes1[2].set_ylabel('Effective Current (μA)')\n",
        "axes1[2].set_title('Effective Drive (Bias - Adaptation)')\n",
        "axes1[2].grid(True, linestyle='--', alpha=0.7)\n",
        "axes1[2].legend(loc='upper left')\n",
        "\n",
        "# 4. Overlay: Bias, Adaptation, and Effective Drive\n",
        "axes1[3].plot(time*1e9, bias_values*1e6, color='royalblue', linewidth=2, label='Bias')\n",
        "axes1[3].plot(time*1e9, I_adapt*1e6, color='teal', linewidth=2, label='Adaptation')\n",
        "axes1[3].plot(time*1e9, I_effective*1e6, color='darkmagenta', linewidth=2, label='Effective Drive')\n",
        "axes1[3].set_xlabel('Time (ns)')\n",
        "axes1[3].set_ylabel('Current (μA)')\n",
        "axes1[3].set_title('Overlay: Bias, Adaptation, and Effective Drive')\n",
        "axes1[3].grid(True, linestyle='--', alpha=0.7)\n",
        "axes1[3].legend(loc='upper left')\n",
        "\n",
        "# 5. Unwrapped Phase Dynamics\n",
        "unwrapped_phase = np.unwrap(phi)\n",
        "axes1[4].plot(time*1e9, unwrapped_phase, color='forestgreen', linewidth=2, label='Unwrapped Phase')\n",
        "axes1[4].set_xlabel('Time (ns)')\n",
        "axes1[4].set_ylabel('Phase (rad)')\n",
        "axes1[4].set_title('Unwrapped Phase Dynamics')\n",
        "axes1[4].grid(True, linestyle='--', alpha=0.7)\n",
        "axes1[4].legend(loc='upper left')\n",
        "\n",
        "# 6. Voltage Trace\n",
        "axes1[5].plot(time*1e9, voltage*1e6, color='crimson', linewidth=1.5, label='Voltage')\n",
        "axes1[5].set_xlabel('Time (ns)')\n",
        "axes1[5].set_ylabel('Voltage (μV)')\n",
        "axes1[5].set_title('Voltage Trace')\n",
        "axes1[5].grid(True, linestyle='--', alpha=0.7)\n",
        "axes1[5].legend(loc='upper left')\n",
        "\n",
        "# 7. Voltage Derivative (Spike Detection)\n",
        "axes1[6].plot(time*1e9, voltage_derivative, color='darkorange', linewidth=1.5, label='dV/dt')\n",
        "if len(steady_spike_times) > 0:\n",
        "    axes1[6].plot(steady_spike_times*1e9, np.interp(steady_spike_times, time, voltage_derivative),\n",
        "                  'o', color='gold', markersize=5, label='Detected Spikes')\n",
        "axes1[6].set_xlabel('Time (ns)')\n",
        "axes1[6].set_ylabel('dV/dt (V/s)')\n",
        "axes1[6].set_title('Voltage Derivative (Spike Detection)')\n",
        "axes1[6].grid(True, linestyle='--', alpha=0.7)\n",
        "axes1[6].legend(loc='upper left')\n",
        "\n",
        "# 8. Flux Quanta (from Unwrapped Phase)\n",
        "flux_quanta_over_time = np.abs(unwrapped_phase) / (2 * np.pi)\n",
        "axes1[7].plot(time*1e9, flux_quanta_over_time, color='purple', linewidth=2, label='Flux Quanta')\n",
        "axes1[7].set_xlabel('Time (ns)')\n",
        "axes1[7].set_ylabel('Flux Quanta')\n",
        "axes1[7].set_title('Flux Quanta Emission')\n",
        "axes1[7].grid(True, linestyle='--', alpha=0.7)\n",
        "axes1[7].legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Figure 2: Frequency and ISI Analysis ---\n",
        "fig2, axes2 = plt.subplots(2, 2, figsize=(12, 10), dpi=150, facecolor='white')\n",
        "\n",
        "# f–I Curve (Steady-State)\n",
        "axes2[0,0].scatter(bias_for_frequency*1e6, instantaneous_frequency*1e-9,\n",
        "                    color='magenta', s=50, zorder=3)\n",
        "axes2[0,0].set_xlabel('Bias at Spike (μA)')\n",
        "axes2[0,0].set_ylabel('Frequency (GHz)')\n",
        "axes2[0,0].set_title('f–I Curve (Steady-State Spikes)')\n",
        "axes2[0,0].grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Instantaneous Frequency vs. Time\n",
        "if mid_spike_times.size:\n",
        "    axes2[0,1].plot(mid_spike_times*1e9, instantaneous_frequency*1e-9, 'o-', color='darkred')\n",
        "    axes2[0,1].set_xlabel('Time (ns)')\n",
        "    axes2[0,1].set_ylabel('Frequency (GHz)')\n",
        "    axes2[0,1].set_title('Instantaneous Frequency vs. Time')\n",
        "    axes2[0,1].grid(True, linestyle='--', alpha=0.7)\n",
        "else:\n",
        "    axes2[0,1].text(0.5, 0.5, 'No Frequency Data', horizontalalignment='center', verticalalignment='center')\n",
        "\n",
        "# ISI Histogram (Steady-State)\n",
        "if spike_intervals.size:\n",
        "    axes2[1,0].hist(spike_intervals*1e9, bins=50, color='green', alpha=0.7)\n",
        "    axes2[1,0].set_xlabel('Inter-Spike Interval (ns)')\n",
        "    axes2[1,0].set_ylabel('Count')\n",
        "    axes2[1,0].set_title('ISI Histogram (Steady-State)')\n",
        "    axes2[1,0].grid(True, linestyle='--', alpha=0.7)\n",
        "else:\n",
        "    axes2[1,0].text(0.5, 0.5, 'No ISI Data', horizontalalignment='center', verticalalignment='center')\n",
        "\n",
        "# Effective Drive vs. Instantaneous Frequency at Spike Times\n",
        "if instantaneous_frequency.size:\n",
        "    axes2[1,1].scatter(bias_for_frequency*1e6, instantaneous_frequency*1e-9,\n",
        "                        color='blue', s=50, zorder=3)\n",
        "    axes2[1,1].set_xlabel('Bias at Spike (μA)')\n",
        "    axes2[1,1].set_ylabel('Frequency (GHz)')\n",
        "    axes2[1,1].set_title('Frequency vs. Bias (Steady-State)')\n",
        "    axes2[1,1].grid(True, linestyle='--', alpha=0.7)\n",
        "else:\n",
        "    axes2[1,1].text(0.5, 0.5, 'No Data', horizontalalignment='center', verticalalignment='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Figure 3: Applied Current (Bias) vs Output Voltage ---\n",
        "plt.figure(figsize=(10, 6), dpi=300, facecolor='white')\n",
        "plt.plot(bias_values * 1e6, voltage * 1e6, color='darkblue', linewidth=1.5)\n",
        "plt.xlabel('Applied Current (μA)')\n",
        "plt.ylabel('Output Voltage (μV)')\n",
        "plt.title('Bias Current vs Output Voltage')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oMggvFdOBozP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4x4 Josephson Junction Array**"
      ],
      "metadata": {
        "id": "DfEpweys88Hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
        "plt.rcParams.update({\n",
        "    'font.family': 'DejaVu Sans',\n",
        "    'axes.titlesize': 14,\n",
        "    'axes.labelsize': 12,\n",
        "    'legend.fontsize': 10,\n",
        "    'xtick.labelsize': 10,\n",
        "    'ytick.labelsize': 10\n",
        "})\n",
        "\n",
        "# ================= Optional Accelerators =================\n",
        "# Try to import numba for JIT acceleration.\n",
        "try:\n",
        "    from numba import njit\n",
        "    numba_available = True\n",
        "except ImportError:\n",
        "    numba_available = False\n",
        "\n",
        "# Try to import joblib for parallel processing.\n",
        "try:\n",
        "    from joblib import Parallel, delayed\n",
        "    parallel_available = True\n",
        "except ImportError:\n",
        "    parallel_available = False\n",
        "\n",
        "# ================= Two-Pointer Helper Functions for STDP =================\n",
        "if numba_available:\n",
        "    @njit\n",
        "    def compute_stdp_two_pointer(post_arr, pre_arr, window, A_plus, tau_plus, A_minus, tau_minus):\n",
        "        result = 0.0\n",
        "        n_post = post_arr.shape[0]\n",
        "        n_pre = pre_arr.shape[0]\n",
        "        i = 0\n",
        "        j = 0\n",
        "        while i < n_post:\n",
        "            while j < n_pre and pre_arr[j] < post_arr[i] - window:\n",
        "                j += 1\n",
        "            k = j\n",
        "            while k < n_pre and pre_arr[k] <= post_arr[i] + window:\n",
        "                dt = post_arr[i] - pre_arr[k]\n",
        "                if dt > 0:\n",
        "                    result += A_plus * np.exp(-dt / tau_plus)\n",
        "                elif dt < 0:\n",
        "                    result -= A_minus * np.exp(dt / tau_minus)\n",
        "                k += 1\n",
        "            i += 1\n",
        "        return result\n",
        "\n",
        "    @njit\n",
        "    def compute_hebbian_two_pointer(post_arr, pre_arr, window):\n",
        "        count = 0\n",
        "        n_post = post_arr.shape[0]\n",
        "        n_pre = pre_arr.shape[0]\n",
        "        i = 0\n",
        "        j = 0\n",
        "        while i < n_post:\n",
        "            while j < n_pre and pre_arr[j] < post_arr[i] - window:\n",
        "                j += 1\n",
        "            k = j\n",
        "            while k < n_pre and pre_arr[k] <= post_arr[i] + window:\n",
        "                count += 1\n",
        "                k += 1\n",
        "            i += 1\n",
        "        return count\n",
        "else:\n",
        "    def compute_stdp_two_pointer(post_arr, pre_arr, window, A_plus, tau_plus, A_minus, tau_minus):\n",
        "        result = 0.0\n",
        "        n_post = post_arr.shape[0]\n",
        "        n_pre = pre_arr.shape[0]\n",
        "        i = 0\n",
        "        j = 0\n",
        "        while i < n_post:\n",
        "            while j < n_pre and pre_arr[j] < post_arr[i] - window:\n",
        "                j += 1\n",
        "            k = j\n",
        "            while k < n_pre and pre_arr[k] <= post_arr[i] + window:\n",
        "                dt = post_arr[i] - pre_arr[k]\n",
        "                if dt > 0:\n",
        "                    result += A_plus * np.exp(-dt / tau_plus)\n",
        "                elif dt < 0:\n",
        "                    result -= A_minus * np.exp(dt / tau_minus)\n",
        "                k += 1\n",
        "            i += 1\n",
        "        return result\n",
        "\n",
        "    def compute_hebbian_two_pointer(post_arr, pre_arr, window):\n",
        "        count = 0\n",
        "        n_post = post_arr.shape[0]\n",
        "        n_pre = pre_arr.shape[0]\n",
        "        i = 0\n",
        "        j = 0\n",
        "        while i < n_post:\n",
        "            while j < n_pre and pre_arr[j] < post_arr[i] - window:\n",
        "                j += 1\n",
        "            k = j\n",
        "            while k < n_pre and pre_arr[k] <= post_arr[i] + window:\n",
        "                count += 1\n",
        "                k += 1\n",
        "            i += 1\n",
        "        return count\n",
        "\n",
        "# ================= Numba-Accelerated ODE Dynamics =================\n",
        "if numba_available:\n",
        "    @njit\n",
        "    def rcsj_dynamics_numba(t, state, N, critical_current, capacitance,\n",
        "                            shunt_resistance, simulation_time, noise_base_amp,\n",
        "                            noise_grid, noise_values, tau_adapt, k_adapt,\n",
        "                            ramp_rate, FLUX_QUANTUM_HALF,\n",
        "                            w_above, w_below, w_left, w_right):\n",
        "        # Reshape state vector into matrices.\n",
        "        phi = state[0:N*N].reshape(N, N)\n",
        "        V = state[N*N:2*N*N].reshape(N, N)\n",
        "        I_adapt = state[2*N*N:].reshape(N, N)\n",
        "        # Clip voltage.\n",
        "        for i in range(N):\n",
        "            for j in range(N):\n",
        "                if V[i, j] < -2e-3:\n",
        "                    V[i, j] = -2e-3\n",
        "                elif V[i, j] > 2e-3:\n",
        "                    V[i, j] = 2e-3\n",
        "\n",
        "        # Compute synaptic current.\n",
        "        I_syn = np.zeros((N, N))\n",
        "        if N > 1:\n",
        "            for i in range(1, N):\n",
        "                for j in range(N):\n",
        "                    I_syn[i, j] += w_above[i, j] * V[i-1, j]\n",
        "            for i in range(N-1):\n",
        "                for j in range(N):\n",
        "                    I_syn[i, j] += w_below[i, j] * V[i+1, j]\n",
        "            for i in range(N):\n",
        "                for j in range(1, N):\n",
        "                    I_syn[i, j] += w_left[i, j] * V[i, j-1]\n",
        "            for i in range(N):\n",
        "                for j in range(N-1):\n",
        "                    I_syn[i, j] += w_right[i, j] * V[i, j+1]\n",
        "        # Clip I_syn.\n",
        "        for i in range(N):\n",
        "            for j in range(N):\n",
        "                if I_syn[i, j] < -2e-5:\n",
        "                    I_syn[i, j] = -2e-5\n",
        "                elif I_syn[i, j] > 2e-5:\n",
        "                    I_syn[i, j] = 2e-5\n",
        "\n",
        "        # Bias current.\n",
        "        bias = ramp_rate * t\n",
        "        if bias > critical_current * 2:\n",
        "            bias = critical_current * 2\n",
        "\n",
        "        # Effective current.\n",
        "        I_eff = np.zeros((N, N))\n",
        "        for i in range(N):\n",
        "            for j in range(N):\n",
        "                I_eff[i, j] = bias + I_syn[i, j] - I_adapt[i, j]\n",
        "                if I_eff[i, j] < -2e-5:\n",
        "                    I_eff[i, j] = -2e-5\n",
        "                elif I_eff[i, j] > 2e-5:\n",
        "                    I_eff[i, j] = 2e-5\n",
        "\n",
        "        # Noise computation.\n",
        "        amp = noise_base_amp * (1 + 0.3 * np.sin(2 * np.pi * t / simulation_time))\n",
        "        noise = np.interp(t, noise_grid, noise_values) * amp\n",
        "        if noise < -noise_base_amp:\n",
        "            noise = -noise_base_amp\n",
        "        elif noise > noise_base_amp:\n",
        "            noise = noise_base_amp\n",
        "\n",
        "        # Derivatives.\n",
        "        dphi_dt = V / FLUX_QUANTUM_HALF\n",
        "        dV_dt = (I_eff - critical_current * np.sin(phi) - V / shunt_resistance) / capacitance + noise\n",
        "        dI_adapt_dt = (k_adapt * np.tanh(V / 1e-6) - I_adapt) / tau_adapt\n",
        "\n",
        "        result = np.empty(state.size)\n",
        "        idx = 0\n",
        "        for i in range(N):\n",
        "            for j in range(N):\n",
        "                result[idx] = dphi_dt[i, j]\n",
        "                idx += 1\n",
        "        for i in range(N):\n",
        "            for j in range(N):\n",
        "                result[idx] = dV_dt[i, j]\n",
        "                idx += 1\n",
        "        for i in range(N):\n",
        "            for j in range(N):\n",
        "                result[idx] = dI_adapt_dt[i, j]\n",
        "                idx += 1\n",
        "        return result\n",
        "\n",
        "    # ---------------- Explicit Euler Integrator (Numba compiled) ----------------\n",
        "    @njit\n",
        "    def euler_integrator(state0, t0, dt, nsteps, N, critical_current, capacitance,\n",
        "                         shunt_resistance, simulation_time, noise_base_amp,\n",
        "                         noise_grid, noise_values, tau_adapt, k_adapt,\n",
        "                         ramp_rate, FLUX_QUANTUM_HALF,\n",
        "                         w_above, w_below, w_left, w_right):\n",
        "        state = state0.copy()\n",
        "        sol = np.empty((nsteps + 1, state0.size))\n",
        "        sol[0, :] = state0\n",
        "        t = t0\n",
        "        for i in range(1, nsteps + 1):\n",
        "            dstate = rcsj_dynamics_numba(t, state, N, critical_current, capacitance,\n",
        "                                         shunt_resistance, simulation_time, noise_base_amp,\n",
        "                                         noise_grid, noise_values, tau_adapt, k_adapt,\n",
        "                                         ramp_rate, FLUX_QUANTUM_HALF,\n",
        "                                         w_above, w_below, w_left, w_right)\n",
        "            state = state + dt * dstate\n",
        "            sol[i, :] = state\n",
        "            t += dt\n",
        "        return sol\n",
        "\n",
        "# ================= The Simulation Class =================\n",
        "class JJArraySimulator:\n",
        "    \"\"\"\n",
        "    Simulates a 2D array of Josephson Junctions.\n",
        "\n",
        "    This version includes:\n",
        "      • A choice of integration methods: either LSODA or a fully compiled explicit Euler integrator.\n",
        "      • Two-pointer algorithms for STDP weight updates.\n",
        "      • Parallel implementations for weight update and chaos analysis.\n",
        "      • Tuned parameters that, for example, (a) reduce the noise and (b) slow the adaptation dynamics.\n",
        "        These changes lessen fluctuations in inter-spike intervals and, in our tests, produce a lower Lyapunov value.\n",
        "    \"\"\"\n",
        "    HBAR = 1.0545718e-34\n",
        "    ELECTRON_CHARGE = 1.60217662e-19\n",
        "    FLUX_QUANTUM_HALF = HBAR / (2 * ELECTRON_CHARGE)\n",
        "\n",
        "    def __init__(self, config, verbose=True, use_numba=False, use_parallel=False):\n",
        "        self.N = config['N']\n",
        "        self.verbose = verbose\n",
        "        self.use_numba = use_numba and numba_available\n",
        "        self.use_parallel = use_parallel and parallel_available\n",
        "        self.integration_method = config.get(\"integration_method\", \"lsoda\")  # \"euler\" or \"lsoda\"\n",
        "        self.state_size = 3 * self.N * self.N\n",
        "        self.critical_current = config['critical_current']\n",
        "        self.capacitance = config['capacitance']\n",
        "        self.shunt_resistance = config['shunt_resistance']\n",
        "        self.simulation_time = config['simulation_time']\n",
        "        self.dt = config['dt']\n",
        "        # For LSODA, create a time grid.\n",
        "        self.time_points = np.linspace(0, self.simulation_time, int(self.simulation_time/self.dt)+1)\n",
        "\n",
        "        # Precompute noise arrays.\n",
        "        self.noise_base_amp = config['noise_amp']\n",
        "        self.noise_grid = np.linspace(0, self.simulation_time, 10000)\n",
        "        self.noise_values = np.random.normal(0.0, self.noise_base_amp, self.noise_grid.shape)\n",
        "\n",
        "        self.tau_adapt = config['tau_adapt']\n",
        "        self.k_adapt = config['k_adapt']\n",
        "        self.learning_params = config['learning_params']\n",
        "\n",
        "        self.weights = self.initialize_weights(config['initial_weight'])\n",
        "        self.ramp_rate = config['ramp_rate']\n",
        "\n",
        "    def initialize_weights(self, initial_weight):\n",
        "        return tuple(np.full((self.N, self.N), initial_weight) for _ in range(4))\n",
        "\n",
        "    def rcsj_dynamics(self, t, state):\n",
        "        # Pure Python fallback (if Numba not used).\n",
        "        phi = state[0:self.N*self.N].reshape(self.N, self.N)\n",
        "        V = state[self.N*self.N: 2*self.N*self.N].reshape(self.N, self.N)\n",
        "        I_adapt = state[2*self.N*self.N:].reshape(self.N, self.N)\n",
        "        V = np.clip(V, -2e-3, 2e-3)\n",
        "        w_above, w_below, w_left, w_right = self.weights\n",
        "        I_syn = np.zeros((self.N, self.N))\n",
        "        if self.N > 1:\n",
        "            I_syn[1:,:] += w_above[1:,:] * V[:-1,:]\n",
        "            I_syn[:-1,:] += w_below[:-1,:] * V[1:,:]\n",
        "            I_syn[:,1:] += w_left[:,1:] * V[:,:-1]\n",
        "            I_syn[:,:-1] += w_right[:,:-1] * V[:,1:]\n",
        "        I_syn = np.clip(I_syn, -2e-5, 2e-5)\n",
        "        bias = np.clip(self.ramp_rate*t, 0, self.critical_current*2)\n",
        "        I_eff = np.clip(bias + I_syn - I_adapt, -2e-5, 2e-5)\n",
        "        amp = self.noise_base_amp*(1+0.3*np.sin(2*np.pi*t/self.simulation_time))\n",
        "        noise = np.clip(np.interp(t, self.noise_grid, self.noise_values)*amp,\n",
        "                        -self.noise_base_amp, self.noise_base_amp)\n",
        "        dphi_dt = V/self.FLUX_QUANTUM_HALF\n",
        "        dV_dt = (I_eff - self.critical_current*np.sin(phi) - V/self.shunt_resistance)/self.capacitance + noise\n",
        "        dI_adapt_dt = (self.k_adapt*np.tanh(V/1e-6)-I_adapt)/self.tau_adapt\n",
        "        return np.concatenate([dphi_dt.flatten(), dV_dt.flatten(), dI_adapt_dt.flatten()])\n",
        "\n",
        "    def run_simulation(self):\n",
        "        \"\"\"Run the simulation using the chosen integration method.\"\"\"\n",
        "        if self.integration_method == \"euler\":\n",
        "            self.run_simulation_euler()\n",
        "        else:\n",
        "            self.run_simulation_lsoda()\n",
        "\n",
        "    def run_simulation_lsoda(self):\n",
        "        \"\"\"Integration using LSODA (with relaxed tolerances).\"\"\"\n",
        "        initial_state = np.zeros(self.state_size)\n",
        "        initial_state[0:self.N*self.N] = np.random.uniform(-0.2, 0.2, self.N*self.N)\n",
        "        if self.use_numba:\n",
        "            def dynamics_jit(t, state):\n",
        "                return rcsj_dynamics_numba(\n",
        "                    t, state, self.N, self.critical_current, self.capacitance,\n",
        "                    self.shunt_resistance, self.simulation_time, self.noise_base_amp,\n",
        "                    self.noise_grid, self.noise_values, self.tau_adapt, self.k_adapt,\n",
        "                    self.ramp_rate, self.FLUX_QUANTUM_HALF,\n",
        "                    self.weights[0], self.weights[1], self.weights[2], self.weights[3]\n",
        "                )\n",
        "            dynamics = dynamics_jit\n",
        "        else:\n",
        "            dynamics = self.rcsj_dynamics\n",
        "        solution = solve_ivp(\n",
        "            dynamics, (0, self.simulation_time), initial_state, method=\"LSODA\",\n",
        "            t_eval=self.time_points, atol=1e-8, rtol=1e-6, max_step=self.dt*10\n",
        "        )\n",
        "        if self.verbose:\n",
        "            print(f\"LSODA status: {solution.status}, Message: {solution.message}\")\n",
        "            print(f\"Function evaluations: {solution.nfev}\")\n",
        "        self.time = solution.t\n",
        "        sol = solution.y\n",
        "        self.phi = sol[0:self.N*self.N].reshape(self.N, self.N, -1)\n",
        "        self.V = sol[self.N*self.N:2*self.N*self.N].reshape(self.N, self.N, -1)\n",
        "        self.I_adapt = sol[2*self.N*self.N:].reshape(self.N, self.N, -1)\n",
        "\n",
        "    def run_simulation_euler(self):\n",
        "        \"\"\"Integration using an explicit Euler method fully compiled with Numba.\"\"\"\n",
        "        nsteps = int(self.simulation_time / self.dt)\n",
        "        initial_state = np.zeros(self.state_size)\n",
        "        initial_state[0:self.N*self.N] = np.random.uniform(-0.2, 0.2, self.N*self.N)\n",
        "        if self.use_numba:\n",
        "            sol = euler_integrator(\n",
        "                initial_state, 0.0, self.dt, nsteps, self.N, self.critical_current, self.capacitance,\n",
        "                self.shunt_resistance, self.simulation_time, self.noise_base_amp, self.noise_grid,\n",
        "                self.noise_values, self.tau_adapt, self.k_adapt, self.ramp_rate, self.FLUX_QUANTUM_HALF,\n",
        "                self.weights[0], self.weights[1], self.weights[2], self.weights[3]\n",
        "            )\n",
        "        else:\n",
        "            sol = np.empty((nsteps+1, self.state_size))\n",
        "            sol[0] = initial_state\n",
        "            state = initial_state.copy()\n",
        "            t = 0.0\n",
        "            for i in range(1, nsteps+1):\n",
        "                dstate = self.rcsj_dynamics(t, state)\n",
        "                state = state + self.dt * dstate\n",
        "                sol[i] = state\n",
        "                t += self.dt\n",
        "        self.time = np.linspace(0, self.simulation_time, nsteps+1)\n",
        "        self.phi = sol[:, 0:self.N*self.N].T.reshape(self.N, self.N, -1)\n",
        "        self.V = sol[:, self.N*self.N:2*self.N*self.N].T.reshape(self.N, self.N, -1)\n",
        "        self.I_adapt = sol[:, 2*self.N*self.N:].T.reshape(self.N, self.N, -1)\n",
        "\n",
        "    def detect_spikes(self, threshold_factor=0.05, min_distance=8):\n",
        "        \"\"\"Detect spikes in each neuron's voltage trace using adaptive thresholds.\"\"\"\n",
        "        spike_times = [[[] for _ in range(self.N)] for _ in range(self.N)]\n",
        "        total_spikes = 0\n",
        "        for i in range(self.N):\n",
        "            for j in range(self.N):\n",
        "                V_ij = self.V[i, j, :]\n",
        "                std_v = np.std(V_ij)\n",
        "                threshold = max(threshold_factor * std_v, 1e-7)\n",
        "                peaks, _ = find_peaks(V_ij, height=threshold, distance=min_distance)\n",
        "                spike_times[i][j] = self.time[peaks]\n",
        "                total_spikes += len(peaks)\n",
        "                if self.verbose:\n",
        "                    print(f\"Neuron ({i},{j}): {len(peaks)} spikes\")\n",
        "        if self.verbose:\n",
        "            print(f\"Total spikes: {total_spikes}\")\n",
        "        return spike_times\n",
        "\n",
        "    # --------------- Weight Update (STDP) ---------------\n",
        "    def update_weights(self, spike_times, rule=\"stdp\"):\n",
        "        if self.use_parallel:\n",
        "            return self._update_weights_parallel(spike_times, rule)\n",
        "        else:\n",
        "            return self._update_weights_sequential(spike_times, rule)\n",
        "\n",
        "    def _update_weights_sequential(self, spike_times, rule=\"stdp\"):\n",
        "        w_above, w_below, w_left, w_right = [w.copy() for w in self.weights]\n",
        "        params = self.learning_params[rule]\n",
        "        window = params.get(\"window\", max(params.get(\"tau_plus\",0), params.get(\"tau_minus\",0))*5)\n",
        "        post_limit = 100\n",
        "        for i in range(self.N):\n",
        "            for j in range(self.N):\n",
        "                post_spikes = spike_times[i][j]\n",
        "                if len(post_spikes)==0:\n",
        "                    continue\n",
        "                post_arr = np.array(post_spikes[:min(post_limit, len(post_spikes))])\n",
        "                for ni, nj, w_matrix, valid in [\n",
        "                    (i-1, j, w_above, i>0),\n",
        "                    (i+1, j, w_below, i<self.N-1),\n",
        "                    (i, j-1, w_left, j>0),\n",
        "                    (i, j+1, w_right, j<self.N-1)\n",
        "                ]:\n",
        "                    if not valid: continue\n",
        "                    pre_spikes = spike_times[ni][nj]\n",
        "                    if len(pre_spikes)==0: continue\n",
        "                    pre_arr = np.array(pre_spikes)\n",
        "                    if rule==\"stdp\":\n",
        "                        Delta_w = compute_stdp_two_pointer(post_arr, pre_arr, window,\n",
        "                                                           params[\"A_plus\"], params[\"tau_plus\"],\n",
        "                                                           params[\"A_minus\"], params[\"tau_minus\"])\n",
        "                    elif rule==\"hebbian\":\n",
        "                        Delta_w = compute_hebbian_two_pointer(post_arr, pre_arr, window)\n",
        "                    elif rule==\"anti_hebbian\":\n",
        "                        Delta_w = - compute_hebbian_two_pointer(post_arr, pre_arr, window)\n",
        "                    w_matrix[i,j] = np.clip(w_matrix[i,j] + params[\"eta\"] * Delta_w, 0, 1.0)\n",
        "        return w_above, w_below, w_left, w_right\n",
        "\n",
        "    def _update_weights_parallel(self, spike_times, rule=\"stdp\"):\n",
        "        params = self.learning_params[rule]\n",
        "        window = params.get(\"window\", max(params.get(\"tau_plus\",0), params.get(\"tau_minus\",0))*5)\n",
        "        post_limit = 100\n",
        "        N = self.N\n",
        "        def update_single(i, j):\n",
        "            post_spikes = spike_times[i][j]\n",
        "            Delta = {\"above\": 0.0, \"below\": 0.0, \"left\": 0.0, \"right\": 0.0}\n",
        "            if len(post_spikes)==0:\n",
        "                return {(i, j): Delta}\n",
        "            post_arr = np.array(post_spikes[:min(post_limit, len(post_spikes))])\n",
        "            if i>0:\n",
        "                pre_arr = np.array(spike_times[i-1][j])\n",
        "                if pre_arr.size>0:\n",
        "                    if rule==\"stdp\":\n",
        "                        Delta[\"above\"] = compute_stdp_two_pointer(post_arr, pre_arr, window,\n",
        "                                                                  params[\"A_plus\"], params[\"tau_plus\"],\n",
        "                                                                  params[\"A_minus\"], params[\"tau_minus\"])\n",
        "                    elif rule==\"hebbian\":\n",
        "                        Delta[\"above\"] = compute_hebbian_two_pointer(post_arr, pre_arr, window)\n",
        "                    elif rule==\"anti_hebbian\":\n",
        "                        Delta[\"above\"] = - compute_hebbian_two_pointer(post_arr, pre_arr, window)\n",
        "            if i < N-1:\n",
        "                pre_arr = np.array(spike_times[i+1][j])\n",
        "                if pre_arr.size>0:\n",
        "                    if rule==\"stdp\":\n",
        "                        Delta[\"below\"] = compute_stdp_two_pointer(post_arr, pre_arr, window,\n",
        "                                                                  params[\"A_plus\"], params[\"tau_plus\"],\n",
        "                                                                  params[\"A_minus\"], params[\"tau_minus\"])\n",
        "                    elif rule==\"hebbian\":\n",
        "                        Delta[\"below\"] = compute_hebbian_two_pointer(post_arr, pre_arr, window)\n",
        "                    elif rule==\"anti_hebbian\":\n",
        "                        Delta[\"below\"] = - compute_hebbian_two_pointer(post_arr, pre_arr, window)\n",
        "            if j>0:\n",
        "                pre_arr = np.array(spike_times[i][j-1])\n",
        "                if pre_arr.size>0:\n",
        "                    if rule==\"stdp\":\n",
        "                        Delta[\"left\"] = compute_stdp_two_pointer(post_arr, pre_arr, window,\n",
        "                                                                 params[\"A_plus\"], params[\"tau_plus\"],\n",
        "                                                                 params[\"A_minus\"], params[\"tau_minus\"])\n",
        "                    elif rule==\"hebbian\":\n",
        "                        Delta[\"left\"] = compute_hebbian_two_pointer(post_arr, pre_arr, window)\n",
        "                    elif rule==\"anti_hebbian\":\n",
        "                        Delta[\"left\"] = - compute_hebbian_two_pointer(post_arr, pre_arr, window)\n",
        "            if j < N-1:\n",
        "                pre_arr = np.array(spike_times[i][j+1])\n",
        "                if pre_arr.size>0:\n",
        "                    if rule==\"stdp\":\n",
        "                        Delta[\"right\"] = compute_stdp_two_pointer(post_arr, pre_arr, window,\n",
        "                                                                  params[\"A_plus\"], params[\"tau_plus\"],\n",
        "                                                                  params[\"A_minus\"], params[\"tau_minus\"])\n",
        "                    elif rule==\"hebbian\":\n",
        "                        Delta[\"right\"] = compute_hebbian_two_pointer(post_arr, pre_arr, window)\n",
        "                    elif rule==\"anti_hebbian\":\n",
        "                        Delta[\"right\"] = - compute_hebbian_two_pointer(post_arr, pre_arr, window)\n",
        "            return {(i,j): Delta}\n",
        "        tasks = [(i,j) for i in range(N) for j in range(N)]\n",
        "        parallel_results = Parallel(n_jobs=-1)(delayed(update_single)(i,j) for (i,j) in tasks)\n",
        "        updates = {}\n",
        "        for res in parallel_results:\n",
        "            updates.update(res)\n",
        "        w_above, w_below, w_left, w_right = [w.copy() for w in self.weights]\n",
        "        eta = params[\"eta\"]\n",
        "        for i in range(N):\n",
        "            for j in range(N):\n",
        "                cell = updates.get((i,j), {\"above\":0.0, \"below\":0.0, \"left\":0.0, \"right\":0.0})\n",
        "                if i>0:\n",
        "                    w_above[i,j] = np.clip(w_above[i,j] + eta * cell[\"above\"], 0, 1.0)\n",
        "                if i < N-1:\n",
        "                    w_below[i,j] = np.clip(w_below[i,j] + eta * cell[\"below\"], 0, 1.0)\n",
        "                if j>0:\n",
        "                    w_left[i,j] = np.clip(w_left[i,j] + eta * cell[\"left\"], 0, 1.0)\n",
        "                if j < N-1:\n",
        "                    w_right[i,j] = np.clip(w_right[i,j] + eta * cell[\"right\"], 0, 1.0)\n",
        "        return w_above, w_below, w_left, w_right\n",
        "\n",
        "    # --------------- Chaos Analysis (ISI Metrics) ---------------\n",
        "    def analyze_chaos(self, spike_times):\n",
        "        if self.use_parallel:\n",
        "            return self._analyze_chaos_parallel(spike_times)\n",
        "        else:\n",
        "            return self._analyze_chaos_sequential(spike_times)\n",
        "\n",
        "    def _analyze_chaos_sequential(self, spike_times):\n",
        "        isi_means = np.full((self.N, self.N), np.nan)\n",
        "        isi_stds = np.full((self.N, self.N), np.nan)\n",
        "        isi_cvs = np.full((self.N, self.N), np.nan)\n",
        "        lyapunov_approx = np.full((self.N, self.N), np.nan)\n",
        "        for i in range(self.N):\n",
        "            for j in range(self.N):\n",
        "                spikes = spike_times[i][j]\n",
        "                if len(spikes)>1:\n",
        "                    sp_arr = np.array(spikes)\n",
        "                    isi = np.diff(sp_arr)\n",
        "                    mean_isi = np.mean(isi)\n",
        "                    std_isi = np.std(isi)\n",
        "                    cv = std_isi/mean_isi if mean_isi>0 else 0\n",
        "                    isi_means[i,j] = mean_isi*1e12\n",
        "                    isi_stds[i,j] = std_isi*1e12\n",
        "                    isi_cvs[i,j] = cv\n",
        "                    if len(isi)>2:\n",
        "                        delta_isi = np.abs(np.diff(isi))\n",
        "                        norm_delta = np.clip(delta_isi/(mean_isi+1e-15), 0, 10)\n",
        "                        lyap = np.mean(np.log1p(norm_delta))/(mean_isi*1e9+1e-15)\n",
        "                        lyapunov_approx[i,j] = lyap\n",
        "                if self.verbose:\n",
        "                    print(f\"Neuron ({i},{j}): CV = {isi_cvs[i,j]:.3f}, Lyapunov = {lyapunov_approx[i,j]:.3f}\")\n",
        "        mean_cv = np.nanmean(isi_cvs) if np.any(~np.isnan(isi_cvs)) else 0\n",
        "        mean_lyap = np.nanmean(lyapunov_approx) if np.any(~np.isnan(lyapunov_approx)) else 0\n",
        "        if self.verbose:\n",
        "            print(f\"Average ISI CV: {mean_cv:.3f}, Average Lyapunov: {mean_lyap:.3f}\")\n",
        "        return isi_means, isi_stds, isi_cvs, mean_cv, mean_lyap\n",
        "\n",
        "    def _analyze_chaos_parallel(self, spike_times):\n",
        "        N = self.N\n",
        "        def process_cell(i, j):\n",
        "            spikes = spike_times[i][j]\n",
        "            if len(spikes)>1:\n",
        "                sp_arr = np.array(spikes)\n",
        "                isi = np.diff(sp_arr)\n",
        "                mean_isi = np.mean(isi)\n",
        "                std_isi = np.std(isi)\n",
        "                cv = std_isi/mean_isi if mean_isi>0 else 0.0\n",
        "                lyap = np.nan\n",
        "                if len(isi)>2:\n",
        "                    delta_isi = np.abs(np.diff(isi))\n",
        "                    norm_delta = np.clip(delta_isi/(mean_isi+1e-15), 0, 10)\n",
        "                    lyap = np.mean(np.log1p(norm_delta))/(mean_isi*1e9+1e-15)\n",
        "                return (i, j, mean_isi*1e12, std_isi*1e12, cv, lyap)\n",
        "            else:\n",
        "                return (i, j, np.nan, np.nan, np.nan, np.nan)\n",
        "        tasks = [(i,j) for i in range(N) for j in range(N)]\n",
        "        results = Parallel(n_jobs=-1)(delayed(process_cell)(i,j) for (i,j) in tasks)\n",
        "        isi_means = np.full((N,N), np.nan)\n",
        "        isi_stds = np.full((N,N), np.nan)\n",
        "        isi_cvs = np.full((N,N), np.nan)\n",
        "        lyapunov_approx = np.full((N,N), np.nan)\n",
        "        for (i,j,mean_val, std_val, cv, lyap) in results:\n",
        "            isi_means[i,j] = mean_val\n",
        "            isi_stds[i,j] = std_val\n",
        "            isi_cvs[i,j] = cv\n",
        "            lyapunov_approx[i,j] = lyap\n",
        "        mean_cv = np.nanmean(isi_cvs) if np.any(~np.isnan(isi_cvs)) else 0\n",
        "        mean_lyap = np.nanmean(lyapunov_approx) if np.any(~np.isnan(lyapunov_approx)) else 0\n",
        "        if self.verbose:\n",
        "            for i in range(N):\n",
        "                for j in range(N):\n",
        "                    print(f\"Neuron ({i},{j}): CV: {isi_cvs[i,j]:.3f}, Lyapunov: {lyapunov_approx[i,j]:.3f}\")\n",
        "            print(f\"Average ISI CV: {mean_cv:.3f}, Average Lyapunov: {mean_lyap:.3f}\")\n",
        "        return isi_means, isi_stds, isi_cvs, mean_cv, mean_lyap\n",
        "\n",
        "    # --------------- Spike Frequency Encoding ---------------\n",
        "    def compute_frequency_encoding(self, spike_times):\n",
        "        frequencies = np.zeros((self.N, self.N))\n",
        "        for i in range(self.N):\n",
        "            for j in range(self.N):\n",
        "                spikes = spike_times[i][j]\n",
        "                if len(spikes)>1:\n",
        "                    duration = spikes[-1]-spikes[0]\n",
        "                    frequencies[i,j] = (len(spikes)-1)/duration if duration>0 else 0\n",
        "        return frequencies\n",
        "\n",
        "    # ------------------- Plotting -------------------\n",
        "    def plot_results(self, spike_times, weights_stdp, frequencies, isi_means, isi_stds, isi_cvs):\n",
        "        # Figure 1: voltage trace, raster, weight & frequency maps, histogram.\n",
        "        fig, axes = plt.subplots(5,1, figsize=(10,20), dpi=1200, facecolor='white')\n",
        "        i,j = 0,0\n",
        "        axes[0].plot(self.time*1e9, self.V[i,j,:]*1e6,\n",
        "                     color='crimson', label=f'Voltage ({i},{j})')\n",
        "        axes[0].set_xlabel('Time (ns)')\n",
        "        axes[0].set_ylabel('Voltage (μV)')\n",
        "        axes[0].set_title(f'Voltage Trace (Neuron {i},{j})')\n",
        "        axes[0].grid(True)\n",
        "        axes[0].legend()\n",
        "        all_raster = []\n",
        "        for i_idx in range(self.N):\n",
        "            for j_idx in range(self.N):\n",
        "                all_raster.append(self.time[np.searchsorted(self.time, spike_times[i_idx][j_idx])])\n",
        "        axes[1].eventplot(all_raster, colors='black')\n",
        "        axes[1].set_xlabel('Time (ns)')\n",
        "        axes[1].set_ylabel('Neuron Index')\n",
        "        axes[1].set_title('Spike Raster Plot')\n",
        "        axes[1].grid(True)\n",
        "        im = axes[2].imshow(weights_stdp[0], cmap='hot', interpolation='nearest')\n",
        "        axes[2].set_title('STDP Weights (From Above)')\n",
        "        axes[2].set_xlabel('j')\n",
        "        axes[2].set_ylabel('i')\n",
        "        fig.colorbar(im, ax=axes[2], label='Weight')\n",
        "        im_freq = axes[3].imshow(frequencies*1e-9, cmap='viridis', interpolation='nearest')\n",
        "        axes[3].set_title('Spike Frequencies (GHz)')\n",
        "        axes[3].set_xlabel('j')\n",
        "        axes[3].set_ylabel('i')\n",
        "        fig.colorbar(im_freq, ax=axes[3], label='Frequency (GHz)')\n",
        "        isi_variability = [isi_cvs[i,j] for i in range(self.N)\n",
        "                           for j in range(self.N) if not np.isnan(isi_cvs[i,j])]\n",
        "        axes[4].hist(isi_variability, bins=20, color='purple', alpha=0.7)\n",
        "        axes[4].set_xlabel('ISI Coefficient of Variation')\n",
        "        axes[4].set_ylabel('Count')\n",
        "        axes[4].set_title('ISI Variability (Chaos Indicator)')\n",
        "        axes[4].grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('jj_array_results.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Figure 2: Heatmaps for ISI metrics.\n",
        "        fig, axes = plt.subplots(1,3, figsize=(18,6), dpi=1200)\n",
        "        sns.heatmap(isi_means, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=axes[0])\n",
        "        axes[0].set_title(\"Mean ISI (ps)\")\n",
        "        axes[0].set_xlabel(\"Neuron Column\")\n",
        "        axes[0].set_ylabel(\"Neuron Row\")\n",
        "        sns.heatmap(isi_stds, annot=True, fmt=\".2f\", cmap=\"Oranges\", ax=axes[1])\n",
        "        axes[1].set_title(\"ISI Std Dev (ps)\")\n",
        "        axes[1].set_xlabel(\"Neuron Column\")\n",
        "        axes[1].set_ylabel(\"Neuron Row\")\n",
        "        sns.heatmap(isi_cvs, annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=axes[2])\n",
        "        axes[2].set_title(\"ISI CV\")\n",
        "        axes[2].set_xlabel(\"Neuron Column\")\n",
        "        axes[2].set_ylabel(\"Neuron Row\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('isi_heatmaps.png')\n",
        "        plt.close()\n",
        "\n",
        "# ================= Main Script =================\n",
        "config = {\n",
        "    'N': 4,\n",
        "    'critical_current': 5e-6,    # Larger voltage.\n",
        "    'capacitance': 1e-11,\n",
        "    'shunt_resistance': 30,      # Increase slight damping.\n",
        "    'initial_weight': 0.7,       # Strong coupling.\n",
        "    'simulation_time': 30e-9,\n",
        "    'dt': 1e-13,\n",
        "    'noise_amp': 1e-7,           # Reduced noise amplitude for lower variability.\n",
        "    'tau_adapt': 5e-9,           # Increased adaptation time constant (slower adaptation reduces chaos).\n",
        "    'k_adapt': 0.05,\n",
        "    'ramp_rate': 5e-6 / 30e-9,   # Bias ramp.\n",
        "    'learning_params': {\n",
        "        'stdp': {'A_plus': 0.01, 'A_minus': 0.01,\n",
        "                 'tau_plus': 5e-10, 'tau_minus': 5e-10, 'eta': 0.01},\n",
        "        'hebbian': {'window': 5e-10, 'eta': 0.01},\n",
        "        'anti_hebbian': {'window': 5e-10, 'eta': 0.01}\n",
        "    },\n",
        "    \"integration_method\": \"euler\"  # Use Euler for fast fixed-step integration.\n",
        "}\n",
        "\n",
        "# Toggle verbose, Numba, and parallel options.\n",
        "simulator = JJArraySimulator(config, verbose=False, use_numba=True, use_parallel=True)\n",
        "simulator.run_simulation()\n",
        "spike_times = simulator.detect_spikes()\n",
        "weights_stdp = simulator.update_weights(spike_times, rule=\"stdp\")\n",
        "isi_means, isi_stds, isi_cvs, chaos_cv, chaos_lyap = simulator.analyze_chaos(spike_times)\n",
        "frequencies = simulator.compute_frequency_encoding(spike_times)\n",
        "simulator.plot_results(spike_times, weights_stdp, frequencies, isi_means, isi_stds, isi_cvs)\n",
        "print(f\"Simulation completed. Chaos Metrics - CV: {chaos_cv:.3f}, Lyapunov: {chaos_lyap:.3f}\")\n"
      ],
      "metadata": {
        "id": "vU0G3h9w9Ooa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6), dpi=1200)\n",
        "\n",
        "# Unified font settings\n",
        "label_fontsize = 24\n",
        "tick_fontsize = 20\n",
        "title_fontsize = 24\n",
        "annot_fontsize = 20\n",
        "\n",
        "def set_all_fonts(ax):\n",
        "    \"\"\"Ensure font consistency across labels, titles, and ticks.\"\"\"\n",
        "    ax.set_title(ax.get_title(), fontsize=title_fontsize)\n",
        "    ax.set_xlabel(ax.get_xlabel(), fontsize=label_fontsize)\n",
        "    ax.set_ylabel(ax.get_ylabel(), fontsize=label_fontsize)\n",
        "    ax.tick_params(axis=\"both\", labelsize=tick_fontsize)\n",
        "\n",
        "# Plot 1: Mean ISI\n",
        "heatmap1 = sns.heatmap(isi_means, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=axes[0], annot_kws={\"fontsize\": annot_fontsize}, cbar_kws={\"aspect\": 20})\n",
        "axes[0].set_title(\"Mean ISI (ps)\")\n",
        "axes[0].set_xlabel(\"Neuron Column\")\n",
        "axes[0].set_ylabel(\"Neuron Row\")\n",
        "set_all_fonts(axes[0])\n",
        "\n",
        "# Adjust colorbar font size\n",
        "heatmap1_colorbar = heatmap1.collections[0].colorbar\n",
        "heatmap1_colorbar.ax.tick_params(labelsize=tick_fontsize)\n",
        "\n",
        "# Plot 2: ISI Std Dev\n",
        "heatmap2 = sns.heatmap(isi_stds, annot=True, fmt=\".2f\", cmap=\"Oranges\", ax=axes[1], annot_kws={\"fontsize\": annot_fontsize}, cbar_kws={\"aspect\": 20})\n",
        "axes[1].set_title(\"ISI Std Dev (ps)\")\n",
        "axes[1].set_xlabel(\"Neuron Column\")\n",
        "axes[1].set_ylabel(\"Neuron Row\")\n",
        "set_all_fonts(axes[1])\n",
        "\n",
        "# Adjust colorbar font size\n",
        "heatmap2_colorbar = heatmap2.collections[0].colorbar\n",
        "heatmap2_colorbar.ax.tick_params(labelsize=tick_fontsize)\n",
        "\n",
        "# Plot 3: ISI CV\n",
        "heatmap3 = sns.heatmap(isi_cvs, annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=axes[2], annot_kws={\"fontsize\": annot_fontsize}, cbar_kws={\"aspect\": 20})\n",
        "axes[2].set_title(\"ISI CV\")\n",
        "axes[2].set_xlabel(\"Neuron Column\")\n",
        "axes[2].set_ylabel(\"Neuron Row\")\n",
        "set_all_fonts(axes[2])\n",
        "\n",
        "# Adjust colorbar font size\n",
        "heatmap3_colorbar = heatmap3.collections[0].colorbar\n",
        "heatmap3_colorbar.ax.tick_params(labelsize=tick_fontsize)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('isi_heatmaps_large_fonts.png', transparent=True)\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "nKUKrgbFO5FH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}